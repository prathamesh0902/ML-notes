{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f751f36f-d520-4fff-843c-ef93beedd693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UVwTwq7mxsaF"
   },
   "source": [
    "---\n",
    "\n",
    "## ðŸ“˜ Statistics Overview\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ **1. Import Required Libraries**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¢ **2. Sample Dataset**\n",
    "\n",
    "```python\n",
    "data = pd.DataFrame({\n",
    "    \"Scores\": [55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "})\n",
    "\n",
    "data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š **3. Descriptive Statistics**\n",
    "\n",
    "Mean, median, and mode summarize the central tendency of the data, helping us understand the typical or most common value in the dataset.\n",
    "\n",
    "### âž¤ Mean (Average)\n",
    "\n",
    "```python\n",
    "data[\"Scores\"].mean()\n",
    "```\n",
    "\n",
    "### âž¤ Median\n",
    "\n",
    "```python\n",
    "data[\"Scores\"].median()\n",
    "```\n",
    "\n",
    "### âž¤ Mode\n",
    "\n",
    "```python\n",
    "data[\"Scores\"].mode()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ˆ **4. Measures of Dispersion**\n",
    "\n",
    "Variance, standard deviation, and range describe how spread out the data is, indicating the level of consistency or variability in observations.\n",
    "\n",
    "### âž¤ Variance\n",
    "\n",
    "```python\n",
    "data[\"Scores\"].var()\n",
    "```\n",
    "\n",
    "### âž¤ Standard Deviation\n",
    "\n",
    "```python\n",
    "data[\"Scores\"].std()\n",
    "```\n",
    "\n",
    "### âž¤ Range\n",
    "\n",
    "```python\n",
    "data[\"Scores\"].max() - data[\"Scores\"].min()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ **5. Quartiles & Percentiles**\n",
    "\n",
    "Quartiles divide the data into four equal parts, showing how values are distributed and helping identify skewness and outliers.\n",
    "\n",
    "```python\n",
    "data[\"Scores\"].quantile([0.25, 0.5, 0.75])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‰ **6. Data Distribution Visualization**\n",
    "\n",
    "### âž¤ Histogram\n",
    "\n",
    "```python\n",
    "plt.hist(data[\"Scores\"], bins=5)\n",
    "plt.xlabel(\"Scores\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Score Distribution\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### âž¤ Box Plot\n",
    "\n",
    "```python\n",
    "sns.boxplot(x=data[\"Scores\"])\n",
    "plt.title(\"Box Plot of Scores\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ **7. Probability Basics**\n",
    "\n",
    "### âž¤ Probability of Score â‰¥ 80\n",
    "\n",
    "```python\n",
    "probability = len(data[data[\"Scores\"] >= 80]) / len(data)\n",
    "probability\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¬ **8. Inferential Statistics**\n",
    "\n",
    "Sampling allows us to analyze a subset of data to estimate population characteristics while reducing computational cost.\n",
    "\n",
    "### âž¤ Sampling\n",
    "\n",
    "```python\n",
    "sample = data[\"Scores\"].sample(5, random_state=1)\n",
    "sample\n",
    "```\n",
    "\n",
    "### âž¤ Sample Mean vs Population Mean\n",
    "\n",
    "```python\n",
    "sample.mean(), data[\"Scores\"].mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª **9. Hypothesis Testing (t-test)**\n",
    "\n",
    "Hypothesis testing determines whether observed results are statistically significant or likely due to random chance.\n",
    "\n",
    "### âž¤ One-Sample t-test\n",
    "\n",
    "```python\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "t_stat, p_value = ttest_1samp(data[\"Scores\"], popmean=70)\n",
    "t_stat, p_value\n",
    "```\n",
    "\n",
    "ðŸ“Œ **Interpretation**\n",
    "\n",
    "* If `p-value < 0.05` â†’ Reject Null Hypothesis\n",
    "* Else â†’ Fail to reject\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— **10. Correlation**\n",
    "\n",
    "Correlation quantifies the strength and direction of the relationship between two variables but does not imply causation.\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame({\n",
    "    \"Hours_Studied\": [1,2,3,4,5,6,7,8,9,10],\n",
    "    \"Scores\": [50,55,60,65,70,75,80,85,90,95]\n",
    "})\n",
    "\n",
    "df.corr()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ˆ **11. Simple Linear Regression**\n",
    "\n",
    "Linear regression models the relationship between variables and enables prediction of outcomes based on input features.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = df[[\"Hours_Studied\"]]\n",
    "y = df[\"Scores\"]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "model.coef_, model.intercept_\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "495d7bcb-d594-45e8-b007-ff7b2e5991a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Tx9EIJzfRO1V"
   },
   "source": [
    "# T F and Anova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2533ac2-80cc-43ef-8262-824f19231c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9Z-Djt9SKV2k"
   },
   "source": [
    "Certainly! Here's a complete example that covers T-test, F-test (for two groups), and ANOVA (analysis of variance) tests, including null and alternative hypotheses, p-values, and interpretation of results. We'll use Python and the SciPy library for these statistical tests.\n",
    "\n",
    "### T-Test Example:\n",
    "\n",
    "Let's assume we have two groups of students, Group A and Group B, and we want to test if there's a significant difference in their exam scores. We set our significance level (alpha) to 0.05.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data for exam scores\n",
    "group_a_scores = [85, 88, 92, 78, 90, 82, 87, 95, 88, 91]\n",
    "group_b_scores = [72, 78, 68, 75, 84, 79, 70, 73, 77, 71]\n",
    "\n",
    "# Perform a two-sample T-test\n",
    "t_statistic, p_value = stats.ttest_ind(group_a_scores, group_b_scores)\n",
    "\n",
    "# Define null and alternative hypotheses\n",
    "# Null Hypothesis (H0): There is no significant difference in the exam scores between Group A and Group B.\n",
    "# Alternative Hypothesis (H1): There is a significant difference in the exam scores between Group A and Group B.\n",
    "\n",
    "# Check the p-value against alpha (0.05)\n",
    "alpha = 0.05\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in exam scores.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in exam scores.\")\n",
    "```\n",
    "\n",
    "**Interpretation**: In this T-test, if the p-value is less than our chosen significance level (alpha), which is 0.05, we reject the null hypothesis. In this case, we reject the null hypothesis, suggesting that there is a significant difference in exam scores between Group A and Group B.\n",
    "\n",
    "### F-Test (Two Groups) Example:\n",
    "\n",
    "Now, let's consider an F-test for comparing variances between two groups. We'll use the same groups as above.\n",
    "\n",
    "```python\n",
    "# Perform an F-test for comparing variances\n",
    "f_statistic, p_value = stats.levene(group_a_scores, group_b_scores)\n",
    "\n",
    "# Define null and alternative hypotheses\n",
    "# Null Hypothesis (H0): The variances of exam scores in Group A and Group B are equal.\n",
    "# Alternative Hypothesis (H1): The variances of exam scores in Group A and Group B are not equal.\n",
    "\n",
    "# Check the p-value against alpha (0.05)\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. Variances are not equal between the groups.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. Variances are equal between the groups.\")\n",
    "```\n",
    "\n",
    "**Interpretation**: In this F-test, if the p-value is less than our chosen significance level (alpha), we reject the null hypothesis, indicating that the variances of exam scores in Group A and Group B are not equal.\n",
    "\n",
    "### ANOVA Test Example:\n",
    "\n",
    "Now, let's perform an ANOVA test to compare exam scores among multiple groups.\n",
    "\n",
    "```python\n",
    "# Sample data for exam scores in three groups\n",
    "group_1_scores = [80, 85, 88, 78, 82]\n",
    "group_2_scores = [72, 76, 80, 78, 75]\n",
    "group_3_scores = [90, 92, 85, 88, 91]\n",
    "\n",
    "# Perform a one-way ANOVA test\n",
    "f_statistic, p_value = stats.f_oneway(group_1_scores, group_2_scores, group_3_scores)\n",
    "\n",
    "# Define null and alternative hypotheses\n",
    "# Null Hypothesis (H0): There is no significant difference in exam scores among the groups.\n",
    "# Alternative Hypothesis (H1): There is a significant difference in exam scores among the groups.\n",
    "\n",
    "# Check the p-value against alpha (0.05)\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in exam scores among the groups.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in exam scores among the groups.\")\n",
    "```\n",
    "\n",
    "**Interpretation**: In this ANOVA test, if the p-value is less than our chosen significance level (alpha), we reject the null hypothesis, suggesting that there is a significant difference in exam scores among the groups.\n",
    "\n",
    "These examples cover common hypothesis tests (T-test, F-test, and ANOVA) and demonstrate how to set up null and alternative hypotheses, calculate p-values, and interpret the results based on a significance level of 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab63440e-000a-419f-92f3-3f222302227e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "28MPnGMBixZN"
   },
   "source": [
    "# BV trade off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c3b1129-ae5a-468a-99b1-bd225c929d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "rH1XlTcWl8Yx"
   },
   "source": [
    "[1.2](http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/#:~:text=Varia,E%5B%CB%86y%5D)\n",
    "\n",
    "\n",
    "[Understanding the Bias-Variance Tradeoff - ](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)Seema Singh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa2fdde1-acbf-4b4c-b56f-30cfa92fa5b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2-Wpbrh0vZCN"
   },
   "source": [
    "# Model Evaluation: Overview\n",
    "Model evaluation is the process of assessing how well a machine learning model performs. It ensures the modelâ€™s predictions are accurate and reliable, whether for regression or classification tasks. Proper evaluation helps select the best model, fine-tune hyperparameters, and avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation for Regression Models:\n",
    "Regression models predict continuous numerical outputs. Common metrics include:\n",
    "\n",
    "1. **Mean Absolute Error (MAE):**\n",
    "   Measures the average absolute difference between predicted and actual values.\n",
    "   - Formula:  \n",
    "     \n",
    "     $MAE = \\frac{\\sum_{i=1}^{n} |y_i - \\hat{y}_i|}{n}$\n",
    "     \n",
    "\n",
    "2. **Mean Squared Error (MSE):**\n",
    "   Penalizes larger errors more heavily by squaring the differences.\n",
    "   - Formula:  \n",
    "     \n",
    "     $MSE = \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n}$\n",
    "     \n",
    "\n",
    "3. **Root Mean Squared Error (RMSE):**\n",
    "   Square root of MSE, providing error in the same units as the target variable.\n",
    "   - Formula:  \n",
    "     \n",
    "     $RMSE = \\sqrt{MSE}$\n",
    "     \n",
    "\n",
    "4. **R-squared ($R^2$):**\n",
    "   Indicates how much variance in the target variable is explained by the model.  \n",
    "   - Formula:  \n",
    "     $\n",
    "     R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "     $\n",
    "\n",
    "5. **Adjusted \\(R^2\\):**\n",
    "   Accounts for the number of predictors in the model to avoid overestimation.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation for Classification Models:\n",
    "Classification models predict discrete labels or categories. Key metrics include:\n",
    "\n",
    "1. **Accuracy:**\n",
    "   Proportion of correctly classified instances.\n",
    "   - Formula:  \n",
    "     $\n",
    "     Accuracy = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Samples}}\n",
    "     $\n",
    "\n",
    "2. **Precision:**\n",
    "   Measures the proportion of positive predictions that are correct.\n",
    "   - Formula:  \n",
    "     $\n",
    "     Precision = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "     $\n",
    "\n",
    "3. **Recall (Sensitivity or TPR):**\n",
    "   Measures the proportion of actual positives identified correctly.\n",
    "   - Formula:  \n",
    "     $\n",
    "     Recall = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "     $\n",
    "\n",
    "4. **F1 Score:**\n",
    "   Harmonic mean of precision and recall. Balances the trade-off between them.\n",
    "   - Formula:  \n",
    "     $\n",
    "     F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "     $\n",
    "\n",
    "5. **ROC-AUC:**\n",
    "   Evaluates the trade-off between the True Positive Rate (TPR) and False Positive Rate (FPR) across thresholds.\n",
    "\n",
    "6. **Confusion Matrix:**\n",
    "   A table showing true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "7. **Log Loss:**\n",
    "   Measures the model's probability estimates for classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Techniques for Model Evaluation:\n",
    "1. **Train-Test Split:**  \n",
    "   Divide data into training and testing subsets (e.g., 80/20 split).\n",
    "\n",
    "2. **Cross-Validation:**  \n",
    "   - **k-Fold Cross-Validation:** Split data into \\(k\\) subsets, train on \\(k-1\\), test on 1, and repeat.\n",
    "   - **Stratified k-Fold:** Ensures balanced distribution of classes in each fold (used for imbalanced classification).\n",
    "\n",
    "3. **Nested Cross-Validation:**  \n",
    "   Used for model selection and hyperparameter tuning, avoiding information leakage.\n",
    "\n",
    "4. **Leave-One-Out Cross-Validation (LOOCV):**  \n",
    "   Trains on all data except one instance and repeats for all data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ce3c44e-3431-4ea0-bc89-cd9b15cb7c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "C9H-MiGovZCO"
   },
   "source": [
    "# Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8719a04-1652-4d40-b72e-e0fa42dd5b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4bQvjZLdvZCO"
   },
   "source": [
    "![Type-I-and-Type-II-Errors.webp](attachment:Type-I-and-Type-II-Errors.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20858090-d28a-49b6-a669-e131a3f7224d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NBzGI6BPvZCO"
   },
   "source": [
    "![IMG_1627.jpg](attachment:IMG_1627.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc5e2576-70ad-4b13-a9c2-b44b25ae3008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fP1H6pRvvZCP"
   },
   "source": [
    "![type-i-and-ii-error-2.png](attachment:type-i-and-ii-error-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "280b9f3a-520f-4370-8e2b-719add37a105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "OE8BS35_vZCP"
   },
   "source": [
    "In hypothesis testing, **Type I error** and **Type II error** represent two different types of statistical errors:\n",
    "\n",
    "1. **Type I Error (False Positive)**: This occurs when the null hypothesis is rejected when it is actually true.\n",
    "2. **Type II Error (False Negative)**: This occurs when the null hypothesis is not rejected when it is actually false.\n",
    "\n",
    "### Why We Focus on Type I Error for Confidence Intervals:\n",
    "The reason we focus on **Type I error** when constructing confidence intervals is that we aim to control the probability of making a false positive, that is, rejecting the null hypothesis when it is actually true. Hereâ€™s the reasoning:\n",
    "\n",
    "1. **Significance Level (Î±)**: The confidence level is directly related to the Type I error rate. For example, in a 95% confidence interval, the significance level (Î±) is 0.05, meaning there's a 5% chance of making a Type I error. By controlling the significance level, we are controlling how often we would mistakenly reject the null hypothesis (i.e., how often we would falsely declare a result \"significant\").\n",
    "\n",
    "2. **Controlling Type I Error**: In hypothesis testing, the primary concern is to avoid concluding that an effect exists when it does not (false positive). Therefore, most statistical tests aim to keep the Type I error rate low by setting a pre-specified Î± (e.g., 0.05, 0.01). This is why the significance level, which corresponds to the probability of a Type I error, is chosen as the confidence level in testing.\n",
    "\n",
    "3. **Type II Error (Î²)**: While Type II error is also important, especially in terms of test power, it is often less emphasized in general hypothesis testing frameworks. This is because controlling Type II error (and thus increasing power) usually requires more data. Type II error is related to the probability of failing to reject the null hypothesis when it is actually false, but it is harder to control since it depends on factors like sample size and effect size.\n",
    "\n",
    "4. **Trade-off Between Type I and Type II Errors**: There is an inherent trade-off between Type I and Type II errors. Reducing one typically increases the other. However, in practice, we first control the Type I error (through the confidence level) and then try to reduce the Type II error by increasing the sample size or power of the test.\n",
    "\n",
    "### Why Not Set Type II Error (Î²) to 0?\n",
    "It's practically impossible to reduce Type II error to 0 because doing so would imply detecting every possible effect, no matter how small, which could result in a very high rate of false positives (i.e., an extremely high Type I error). The balance between detecting true effects and avoiding false positives is crucial.\n",
    "\n",
    "Thus, the **confidence interval** is constructed based on **Type I error (Î±)**, not Type II error, because it allows for clear control over the false positive rate and aligns with the goal of avoiding incorrect rejections of the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "692a441f-b23e-491d-8d39-c04c76355504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CK9843etInlB"
   },
   "source": [
    "# Accuracy, Sensitivity, Specificity, ROC, AUC, SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "037ab820-09b5-481d-bbef-5bf5baffc218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "mvP0BI5ErLVj"
   },
   "source": [
    "These terms are commonly used in the evaluation of machine learning models, particularly in classification tasks and model interpretability. Here's an explanation of each:\n",
    "\n",
    "### 1. **Accuracy**\n",
    "   - **Definition**: The proportion of true results (both true positives and true negatives) among the total number of cases examined.\n",
    "   - **Formula**: \\($ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $\\)\n",
    "   - **Use**: Accuracy gives an overall measure of how often the model is correct, but it can be misleading if the classes are imbalanced.\n",
    "\n",
    "### 2. **Sensitivity (Recall or True Positive Rate)**\n",
    "   - **Definition**: The proportion of actual positives that are correctly identified by the model.\n",
    "   - **Formula**: \\($ \\text{Sensitivity} = \\frac{TP}{TP + FN} $\\)\n",
    "   - **Use**: Sensitivity measures how well the model identifies positive cases. Itâ€™s crucial when the cost of missing positive cases is high.\n",
    "\n",
    "### 3. **Specificity (True Negative Rate)**\n",
    "   - **Definition**: The proportion of actual negatives that are correctly identified by the model.\n",
    "   - **Formula**: \\($ \\text{Specificity} = \\frac{TN}{TN + FP} $\\)\n",
    "   - **Use**: Specificity measures how well the model identifies negative cases. Itâ€™s important in situations where false positives are costly.\n",
    "\n",
    "### 4. **Positive Predictive Value (Precision)**\n",
    "   - **Definition**: The proportion of positive results that are true positives.\n",
    "   - **Formula**: \\($ \\text{Precision} = \\frac{TP}{TP + FP} $\\)\n",
    "   - **Use**: Precision indicates how many of the positive predictions are actually correct, which is important when false positives are particularly problematic.\n",
    "\n",
    "### 5. **ROC (Receiver Operating Characteristic) Curve**\n",
    "   - **Definition**: A graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n",
    "   - **Use**: The ROC curve shows the trade-off between sensitivity (true positive rate) and specificity (1 - false positive rate). Itâ€™s useful for comparing the performance of different models.\n",
    "\n",
    "### 6. **AUC (Area Under the Curve)**\n",
    "   - **Definition**: The area under the ROC curve. It provides a single scalar value to summarize the model's performance across all classification thresholds.\n",
    "   - **Use**: AUC represents the likelihood that the model will rank a randomly chosen positive instance higher than a randomly chosen negative one. An AUC of 1 indicates perfect performance, while an AUC of 0.5 indicates no discriminative power.\n",
    "\n",
    "### 7. **Calibration**\n",
    "   - **Definition**: Calibration refers to how well the predicted probabilities of a model match the actual outcomes. A well-calibrated model provides probabilities that reflect the true likelihood of an event.\n",
    "   - **Use**: Calibration is important in probabilistic models where the predicted probability needs to be interpreted directly. Poor calibration can lead to overconfident predictions.\n",
    "\n",
    "### 8. **SHAP (Shapley Additive Explanations)**\n",
    "   - **Definition**: SHAP is a method for interpreting the output of machine learning models. It explains the contribution of each feature to the model's predictions by attributing changes in the prediction to each feature, based on cooperative game theory.\n",
    "   - **Use**: SHAP values help to understand how each feature affects the model's predictions, providing transparency and insights into complex models like ensemble methods or deep learning models. This is particularly useful for ensuring fairness, debugging models, and gaining trust from stakeholders.\n",
    "\n",
    "### Summary:\n",
    "These metrics and concepts are essential for understanding and evaluating the performance and behavior of machine learning models. They provide insights into how well a model is performing, how reliable its predictions are, and how individual features contribute to the outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd074404-e657-4747-92d2-3bc68c255940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DabQ8sC5vZCP"
   },
   "source": [
    "# A/B Testing and Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f8b34d0-8dc7-4e5e-92d6-0070988836dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4z8iAnHCvZCQ"
   },
   "source": [
    "### Case Study: **A/B Testing and Uplift Modeling in Marketing Campaign Optimization**\n",
    "\n",
    "#### **Objective:**\n",
    "A retail company wants to optimize its marketing campaign strategies to maximize customer engagement and sales. The goal is to identify which segment of customers should receive a targeted marketing campaign and to measure the impact of the campaign using A/B testing and uplift modeling.\n",
    "\n",
    "#### **Background:**\n",
    "The retail company regularly runs email marketing campaigns to increase customer purchases. Traditionally, they send out emails to all customers in their database. However, they suspect that not all customers respond equally to the campaigns, and sending emails to uninterested customers could even have a negative impact, such as increasing unsubscribe rates.\n",
    "\n",
    "To address this, the company decides to implement A/B testing combined with uplift modeling. The idea is to identify the customers who are most likely to respond positively to the campaign (i.e., those who are likely to make a purchase after receiving the email) and to send the campaign only to those customers.\n",
    "\n",
    "#### **Data:**\n",
    "The company has historical data on previous email marketing campaigns, including:\n",
    "- **Customer Data:** Demographics, purchase history, engagement metrics (e.g., click-through rates, open rates).\n",
    "- **Campaign Data:** Type of campaign, content of the email, time sent, etc.\n",
    "- **Response Data:** Whether the customer made a purchase after receiving the email, amount spent, unsubscribe rate, etc.\n",
    "\n",
    "#### **Step 1: A/B Testing Setup**\n",
    "1. **Randomization:**\n",
    "   - The customer base is randomly split into two groups: the **treatment group** (who will receive the marketing email) and the **control group** (who will not receive the email).\n",
    "   - The size of each group is carefully chosen to ensure statistical significance.\n",
    "\n",
    "2. **Execution:**\n",
    "   - The marketing campaign is sent to the treatment group, while the control group does not receive any campaign.\n",
    "   - The response is tracked over a defined period, focusing on key metrics like purchase rate, average order value, and unsubscribe rate.\n",
    "\n",
    "3. **Analysis:**\n",
    "   - The difference in outcomes between the treatment and control groups is analyzed to determine the effectiveness of the campaign.\n",
    "   - For instance, if the purchase rate in the treatment group is significantly higher than in the control group, the campaign is deemed successful.\n",
    "\n",
    "#### **Step 2: Uplift Modeling**\n",
    "1. **Modeling Approach:**\n",
    "   - An uplift model is built to predict the individual treatment effect (ITE) for each customer. The model aims to predict the difference in outcome if the customer receives the email versus if they do not.\n",
    "   - Key features include customer demographics, historical purchase behavior, and past engagement with campaigns.\n",
    "\n",
    "2. **Model Training:**\n",
    "   - The uplift model is trained on the historical campaign data, using a technique such as **two-model approach**, **Class Transformation**, or **Qini coefficient** to measure uplift.\n",
    "\n",
    "3. **Customer Segmentation:**\n",
    "   - Customers are segmented into four groups based on the modelâ€™s predictions:\n",
    "     - **Persuadables:** Likely to purchase only if they receive the email.\n",
    "     - **Sure Things:** Likely to purchase regardless of the email.\n",
    "     - **Lost Causes:** Unlikely to purchase, regardless of the email.\n",
    "     - **Do Not Disturbs:** Likely to have a negative response if they receive the email (e.g., unsubscribe or negative brand sentiment).\n",
    "\n",
    "#### **Step 3: Campaign Optimization and Implementation**\n",
    "1. **Targeting:**\n",
    "   - The campaign is refined to focus only on the \"Persuadables\" segment, thereby increasing the likelihood of a positive return on investment (ROI) from the campaign.\n",
    "   - The \"Do Not Disturbs\" are explicitly excluded from the campaign to avoid potential negative impacts.\n",
    "\n",
    "2. **Implementation:**\n",
    "   - The refined campaign is launched, targeting the identified customer segments based on the uplift model predictions.\n",
    "   - A/B testing is conducted again with this refined strategy to validate the modelâ€™s effectiveness.\n",
    "\n",
    "#### **Step 4: Evaluation and Outcome**\n",
    "1. **Key Metrics:**\n",
    "   - The effectiveness of the uplift model is evaluated by comparing the results of the refined campaign to the original A/B test.\n",
    "   - Key metrics include uplift in purchase rate, ROI, and overall customer satisfaction.\n",
    "\n",
    "2. **Results:**\n",
    "   - Suppose the uplift model successfully identified a segment where the purchase rate increased by 20%, while the unsubscribe rate decreased by 15%. This would indicate a more effective campaign compared to traditional blanket marketing.\n",
    "\n",
    "3. **Conclusion:**\n",
    "   - The combination of A/B testing and uplift modeling allows the company to optimize its marketing campaigns, targeting only those customers who are likely to respond positively, thereby improving overall campaign effectiveness and customer experience.\n",
    "\n",
    "#### **Key Takeaways:**\n",
    "- **A/B Testing:** Essential for understanding the baseline effectiveness of marketing campaigns.\n",
    "- **Uplift Modeling:** Provides a more nuanced approach by identifying which customers are most likely to be positively influenced by the campaign, allowing for targeted marketing efforts.\n",
    "- **Outcome:** Increased ROI, better customer engagement, and reduced negative impact (such as unsubscribes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4865e791-8e04-4704-9f3c-19e9b9e8234e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8Qcb5aenvZCQ"
   },
   "source": [
    "In hypothesis testing, Type I and Type II errors represent two different ways that incorrect conclusions can be drawn from statistical tests.\n",
    "\n",
    "### **Type I Error (False Positive)**\n",
    "- **Definition:** A Type I error occurs when the null hypothesis is rejected when it is actually true. In other words, you conclude that there is an effect or difference when there isn't one.\n",
    "- **Probability:** The probability of committing a Type I error is denoted by **Î±** (alpha), which is also the significance level of the test (commonly set at 0.05 or 5%).\n",
    "- **Example:** Imagine testing a new drug to see if it lowers blood pressure. A Type I error would occur if you conclude that the drug works when, in reality, it does not.\n",
    "\n",
    "### **Type II Error (False Negative)**\n",
    "- **Definition:** A Type II error occurs when the null hypothesis is not rejected when it is actually false. This means you fail to detect an effect or difference that is actually present.\n",
    "- **Probability:** The probability of committing a Type II error is denoted by **Î²** (beta). The power of the test (1 - Î²) is the probability of correctly rejecting a false null hypothesis.\n",
    "- **Example:** Using the same drug test example, a Type II error would occur if you conclude that the drug does not lower blood pressure when, in fact, it does.\n",
    "\n",
    "### **Summary:**\n",
    "- **Type I Error:** Rejecting a true null hypothesis (False Positive).\n",
    "- **Type II Error:** Failing to reject a false null hypothesis (False Negative).\n",
    "\n",
    "Both errors are important to consider when designing and interpreting hypothesis tests, as they have implications for the validity of the conclusions drawn from the data."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "STATS Part 2",
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [
    "Acblar5ELNh6",
    "Z3EWknfEewdE",
    "Tx9EIJzfRO1V",
    "B-dk0z-FLNJM"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
